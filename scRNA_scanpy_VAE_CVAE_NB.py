# # å•ç»†èƒžRNAæµ‹åºæ•°æ®åˆ†æžï¼šä»ŽScanpyåˆ°VAE/CVAEå®Œæ•´æµç¨‹
# 
# æœ¬notebookå±•ç¤ºäº†å¦‚ä½•ä½¿ç”¨ä¼ ç»ŸScanpyæ–¹æ³•ä»¥åŠæ·±åº¦å­¦ä¹ VAE/CVAEæ–¹æ³•è¿›è¡Œå•ç»†èƒžæ•°æ®åˆ†æžå’Œæ‰¹æ¬¡æ•ˆåº”æ ¡æ­£

# ## 1. çŽ¯å¢ƒå‡†å¤‡å’Œæ•°æ®åŠ è½½

import os
import sys
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader, TensorDataset
import scanpy as sc
import matplotlib.pyplot as plt
from sklearn.metrics import silhouette_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®Scanpyå‚æ•°
sc.settings.verbosity = 3
sc.settings.set_figure_params(dpi=80, facecolor='white')

# æ·»åŠ é¡¹ç›®è·¯å¾„
BASE_DIR = os.path.dirname(os.path.abspath('__file__'))
PROJ_ROOT = os.path.abspath(os.path.join(BASE_DIR, '..'))
if PROJ_ROOT not in sys.path:
    sys.path.insert(0, PROJ_ROOT)

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from data_processing import make_counts_training_tensors_v2
from VAE_NB.models import VAE
from VAE_NB.training import train_vae
from CVAE_NB.models import CVAE
from CVAE_NB.training import train as train_cvae

print("âœ… çŽ¯å¢ƒé…ç½®å®Œæˆ")

# æ•°æ®è·¯å¾„é…ç½®
DATA_PATH = 'data/GSE149614_HCC_scRNA_test_5N5T_counts.h5ad'
RESULTS_DIR = 'results'
os.makedirs(RESULTS_DIR, exist_ok=True)

# åŠ è½½åŽŸå§‹è®¡æ•°æ•°æ®
adata_counts = sc.read_h5ad(DATA_PATH)
print(f"ðŸ“Š æ•°æ®ç»´åº¦: {adata_counts.shape}")
print(f"ðŸ“‹ ç»†èƒžç±»åž‹: {adata_counts.obs['celltype'].value_counts()}")
print(f"ðŸ§ª æ ·æœ¬ä¿¡æ¯: {adata_counts.obs['sample'].value_counts()}")

# ### å·¥å…·å‡½æ•°å®šä¹‰

def ensure_size_factors(adata):
    """ç¡®ä¿æ•°æ®åŒ…å«size_factorsä¿¡æ¯"""
    if 'size_factors' in adata.obs:
        return
    X = adata.X
    if hasattr(X, 'sum'):
        totals = np.asarray(X.sum(axis=1)).reshape(-1) if not hasattr(X.sum(axis=1), 'A1') else X.sum(axis=1).A1
    else:
        totals = X.sum(axis=1)
    med = np.median(totals[totals>0]) if np.any(totals>0) else 1.0
    sf = totals/(med if med>0 else 1.0)
    sf = np.where(sf<=0, 1.0, sf).astype(np.float32)
    adata.obs['size_factors'] = sf

def standardize_latents(latents):
    """æ ‡å‡†åŒ–æ½œåœ¨è¡¨ç¤º"""
    return (latents - latents.mean(axis=0)) / (latents.std(axis=0) + 1e-8)

def save_training_curves(loss_list, recon_list, kl_list, title, save_path=None):
    """ç»˜åˆ¶å¹¶ä¿å­˜è®­ç»ƒæŸå¤±æ›²çº¿"""
    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 4))
    epochs = range(1, len(loss_list) + 1)
    
    ax1.plot(epochs, loss_list, 'b-', linewidth=2)
    ax1.set_title(f'{title} - Total Loss')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.grid(True, alpha=0.3)
    
    ax2.plot(epochs, recon_list, 'r-', linewidth=2)
    ax2.set_title(f'{title} - Reconstruction Loss')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Reconstruction Loss')
    ax2.grid(True, alpha=0.3)
    
    ax3.plot(epochs, kl_list, 'g-', linewidth=2)
    ax3.set_title(f'{title} - KL Divergence')
    ax3.set_xlabel('Epoch')
    ax3.set_ylabel('KL Divergence')
    ax3.grid(True, alpha=0.3)
    
    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
    return fig

print("ðŸ”§ å·¥å…·å‡½æ•°å®šä¹‰å®Œæˆ")

# ## 2. Scanpyä¼ ç»Ÿåˆ†æžæµç¨‹

def run_scanpy_pipeline(adata_counts, n_top_genes=2000):
    """è¿è¡Œæ ‡å‡†çš„Scanpyå•ç»†èƒžåˆ†æžæµç¨‹"""
    print("ðŸ”¬ å¼€å§‹Scanpyåˆ†æžæµç¨‹...")
    
    adata = adata_counts.copy()
    ensure_size_factors(adata)
    adata.raw = adata
    
    # æ ‡å‡†åŒ–å’Œå¯¹æ•°å˜æ¢
    sc.pp.normalize_total(adata, target_sum=1e4)
    sc.pp.log1p(adata)
    
    # é«˜å˜åŸºå› é€‰æ‹©
    sc.pp.highly_variable_genes(adata, n_top_genes=n_top_genes, flavor='seurat')
    print(f"ðŸ“Š é€‰æ‹©äº† {adata.var['highly_variable'].sum()} ä¸ªé«˜å˜åŸºå› ")
    adata = adata[:, adata.var['highly_variable']].copy()
    
    # æ ‡å‡†åŒ–ã€PCAã€é‚»å±…å›¾ã€UMAP
    sc.pp.scale(adata, max_value=10)
    sc.tl.pca(adata, svd_solver='arpack')
    sc.pp.neighbors(adata, n_neighbors=15, metric='euclidean')
    sc.tl.umap(adata, min_dist=0.2)
    
    # Leidenèšç±»
    try:
        sc.tl.leiden(adata, resolution=1.0)
        print(f"ðŸŽ¯ Leidenèšç±»è¯†åˆ«äº† {len(adata.obs['leiden'].unique())} ä¸ªç°‡")
    except Exception as e:
        print(f"âš ï¸ Leidenèšç±»å¤±è´¥: {e}")
    
    print("âœ… Scanpyåˆ†æžæµç¨‹å®Œæˆ")
    return adata

# è¿è¡ŒScanpyåˆ†æž
adata_scanpy = run_scanpy_pipeline(adata_counts)

# å¯è§†åŒ–Scanpyç»“æžœ
sc.pl.umap(adata_scanpy, color='celltype', save='_scanpy_celltype.png')
sc.pl.umap(adata_scanpy, color='sample', save='_scanpy_sample.png')

# ## 3. Harmonyæ‰¹æ¬¡æ ¡æ­£

def run_harmony_integration(adata, batch_key='sample'):
    """ä½¿ç”¨Harmonyè¿›è¡Œæ‰¹æ¬¡æ ¡æ­£"""
    print("ðŸŽµ å¼€å§‹Harmonyæ‰¹æ¬¡æ ¡æ­£...")
    
    try:
        import scanpy.external as sce
        
        if batch_key not in adata.obs.columns:
            print(f"âš ï¸ æ‰¹æ¬¡é”® '{batch_key}' ä¸å­˜åœ¨ï¼Œè·³è¿‡Harmonyåˆ†æž")
            return adata
        
        # Harmonyæ•´åˆ
        sce.pp.harmony_integrate(adata, key=batch_key, basis='X_pca')
        
        # åŸºäºŽHarmonyç»“æžœé‡æ–°è®¡ç®—UMAP
        sc.pp.neighbors(adata, use_rep='X_pca_harmony', n_neighbors=15, metric='euclidean')
        sc.tl.umap(adata, min_dist=0.2)
        adata.obsm['X_harmony_umap'] = adata.obsm['X_umap'].copy()
        
        # æ¢å¤åŽŸå§‹UMAP
        sc.pp.neighbors(adata, use_rep='X_pca', n_neighbors=15, metric='euclidean')
        sc.tl.umap(adata, min_dist=0.2)
        
        print("âœ… Harmonyåˆ†æžå®Œæˆ")
        
    except ImportError:
        print("âš ï¸ æ— æ³•å¯¼å…¥Harmonyï¼Œè·³è¿‡Harmonyåˆ†æž")
    except Exception as e:
        print(f"âš ï¸ Harmonyåˆ†æžå¤±è´¥: {e}")
    
    return adata

# è¿è¡ŒHarmonyåˆ†æž
adata_scanpy = run_harmony_integration(adata_scanpy)

# å¯è§†åŒ–Harmonyç»“æžœ
if 'X_harmony_umap' in adata_scanpy.obsm:
    sc.pl.embedding(adata_scanpy, basis='X_harmony_umap', color='celltype', save='_harmony_celltype.png')
    sc.pl.embedding(adata_scanpy, basis='X_harmony_umap', color='sample', save='_harmony_sample.png')

# ## 4. VAE_NBæ¨¡åž‹è®­ç»ƒ

def train_vae_model(adata_counts, epochs=50, batch_size=128, lr=1e-3, latent_dim=32):
    """è®­ç»ƒVAE_NBæ¨¡åž‹"""
    print("ðŸ§  å¼€å§‹è®­ç»ƒVAE_NBæ¨¡åž‹...")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X, y_disc, y_cont, sf, x_target, cond_dims, cont_dim = make_counts_training_tensors_v2(
        adata_counts, discrete_key='sample', cont_keys=None, use_hvgs=False, return_dims=True
    )
    
    print(f"ðŸ“Š è®­ç»ƒæ•°æ®ç»´åº¦: {X.shape}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = TensorDataset(X, y_disc, y_cont, sf, x_target)
    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    eval_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    
    # åˆ›å»ºVAEæ¨¡åž‹
    vae_model = VAE(
        input_dim=X.shape[1], latent_dim=latent_dim,
        encoder_layers=[1024, 512, 256], decoder_layers=[256, 512, 1024],
        activation='gelu', norm='layernorm', dropout=0.1, recon_dist='nb'
    )
    
    print(f"ðŸ—ï¸ æ¨¡åž‹å‚æ•°æ•°é‡: {sum(p.numel() for p in vae_model.parameters()):,}")
    
    # è®­ç»ƒæ¨¡åž‹
    optimizer = torch.optim.Adam(vae_model.parameters(), lr=lr)
    loss_list, recon_list, kl_list = train_vae(
        vae_model, train_loader, optimizer, epochs, conditional=False, recon_dist='nb',
        beta_final=0.05, beta_warmup_epochs=epochs, free_bits=0.10, 
        kl_schedule='cosine', kl_period=epochs
    )
    
    # æå–æ½œåœ¨è¡¨ç¤º
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    vae_model = vae_model.to(device)
    vae_model.eval()
    
    latents_list = []
    with torch.no_grad():
        for batch in eval_loader:
            x_batch = batch[0].to(device)
            mu, _ = vae_model.encode(x_batch)
            latents_list.append(mu.cpu().numpy())
    
    latents = np.vstack(latents_list)
    latents_std = standardize_latents(latents)
    
    print("âœ… VAE_NBè®­ç»ƒå®Œæˆ")
    
    # ä¿å­˜è®­ç»ƒæ›²çº¿
    save_training_curves(loss_list, recon_list, kl_list, 'VAE_NB', 
                         save_path=f'{RESULTS_DIR}/vae_nb_training_curves.png')
    plt.show()
    
    return latents, latents_std, {'loss': loss_list, 'recon': recon_list, 'kl': kl_list}

# è®­ç»ƒVAEæ¨¡åž‹
vae_latents, vae_latents_std, vae_loss_history = train_vae_model(adata_counts, epochs=50)

# å°†VAEç»“æžœæ·»åŠ åˆ°æ•°æ®ä¸­å¹¶è®¡ç®—UMAP
adata_scanpy.obsm['X_vae_nb'] = vae_latents
adata_scanpy.obsm['X_vae_nb_std'] = vae_latents_std

sc.pp.neighbors(adata_scanpy, use_rep='X_vae_nb_std', n_neighbors=15, metric='euclidean')
sc.tl.umap(adata_scanpy, min_dist=0.2)
adata_scanpy.obsm['X_vae_umap'] = adata_scanpy.obsm['X_umap'].copy()

# æ¢å¤åŽŸå§‹é‚»å±…å›¾
sc.pp.neighbors(adata_scanpy, use_rep='X_pca', n_neighbors=15, metric='euclidean')
sc.tl.umap(adata_scanpy, min_dist=0.2)

# å¯è§†åŒ–VAEç»“æžœ
sc.pl.embedding(adata_scanpy, basis='X_vae_umap', color='celltype', save='_vae_celltype.png')
sc.pl.embedding(adata_scanpy, basis='X_vae_umap', color='sample', save='_vae_sample.png')

# ## 5. CVAE_NBæ¨¡åž‹è®­ç»ƒ

def train_cvae_model(adata_counts, epochs=50, batch_size=128, lr=1e-3, latent_dim=32):
    """è®­ç»ƒCVAE_NBæ¨¡åž‹ï¼ˆæ¡ä»¶åŒ–æ‰¹æ¬¡ä¿¡æ¯ï¼‰"""
    print("ðŸ§  å¼€å§‹è®­ç»ƒCVAE_NBæ¨¡åž‹...")
    
    # å‡†å¤‡è®­ç»ƒæ•°æ®
    X, y_disc, y_cont, sf, x_target, cond_dims, cont_dim = make_counts_training_tensors_v2(
        adata_counts, discrete_key='sample', cont_keys=None, use_hvgs=False, return_dims=True
    )
    
    print(f"ðŸ“Š è®­ç»ƒæ•°æ®ç»´åº¦: {X.shape}")
    print(f"ðŸ·ï¸ æ¡ä»¶ç»´åº¦: {cond_dims}, {cont_dim}")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataset = TensorDataset(X, y_disc, y_cont, sf, x_target)
    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    eval_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)
    
    # åˆ›å»ºCVAEæ¨¡åž‹
    cvae_model = CVAE(
        input_dim=X.shape[1],
        cond_dim=cond_dims if isinstance(cond_dims, (list, tuple)) else int(cond_dims),
        cont_dim=int(cont_dim), latent_dim=latent_dim,
        encoder_layers=[1024, 512, 256], decoder_layers=[256, 512, 1024],
        activation='gelu', norm='layernorm', dropout=0.1, recon_dist='nb', cond_in_encoder=True
    )
    
    print(f"ðŸ—ï¸ æ¨¡åž‹å‚æ•°æ•°é‡: {sum(p.numel() for p in cvae_model.parameters()):,}")
    
    # è®­ç»ƒæ¨¡åž‹
    optimizer = torch.optim.Adam(cvae_model.parameters(), lr=lr)
    loss_list, recon_list, kl_list = train_cvae(
        cvae_model, train_loader, optimizer, epochs, conditional=True, recon_dist='nb',
        beta_final=0.05, beta_warmup_epochs=epochs, free_bits=0.10, 
        kl_schedule='cosine', kl_period=epochs
    )
    
    # æå–æ½œåœ¨è¡¨ç¤º
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    cvae_model = cvae_model.to(device)
    cvae_model.eval()
    
    latents_list = []
    with torch.no_grad():
        for batch in eval_loader:
            x_batch = batch[0].to(device)
            y_disc_batch = batch[1].to(device)
            y_cont_batch = batch[2].to(device)
            mu, _ = cvae_model.encode(x_batch, y_disc_batch, y_cont_batch)
            latents_list.append(mu.cpu().numpy())
    
    latents = np.vstack(latents_list)
    latents_std = standardize_latents(latents)
    
    print("âœ… CVAE_NBè®­ç»ƒå®Œæˆ")
    
    # ä¿å­˜è®­ç»ƒæ›²çº¿
    save_training_curves(loss_list, recon_list, kl_list, 'CVAE_NB',
                         save_path=f'{RESULTS_DIR}/cvae_nb_training_curves.png')
    plt.show()
    
    return latents, latents_std, {'loss': loss_list, 'recon': recon_list, 'kl': kl_list}

# è®­ç»ƒCVAEæ¨¡åž‹
cvae_latents, cvae_latents_std, cvae_loss_history = train_cvae_model(adata_counts, epochs=50)

# å°†CVAEç»“æžœæ·»åŠ åˆ°æ•°æ®ä¸­å¹¶è®¡ç®—UMAP
adata_scanpy.obsm['X_cvae_nb'] = cvae_latents
adata_scanpy.obsm['X_cvae_nb_std'] = cvae_latents_std

sc.pp.neighbors(adata_scanpy, use_rep='X_cvae_nb_std', n_neighbors=15, metric='euclidean')
sc.tl.umap(adata_scanpy, min_dist=0.2)
adata_scanpy.obsm['X_cvae_umap'] = adata_scanpy.obsm['X_umap'].copy()

# æ¢å¤åŽŸå§‹é‚»å±…å›¾
sc.pp.neighbors(adata_scanpy, use_rep='X_pca', n_neighbors=15, metric='euclidean')
sc.tl.umap(adata_scanpy, min_dist=0.2)

# å¯è§†åŒ–CVAEç»“æžœ
sc.pl.embedding(adata_scanpy, basis='X_cvae_umap', color='celltype', save='_cvae_celltype.png')
sc.pl.embedding(adata_scanpy, basis='X_cvae_umap', color='sample', save='_cvae_sample.png')

# ## 6. ç»“æžœå¯¹æ¯”å’Œå¯è§†åŒ–

# ### è®­ç»ƒæŸå¤±å¯¹æ¯”
if 'vae_loss_history' in locals() and 'cvae_loss_history' in locals():
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    
    epochs_vae = range(1, len(vae_loss_history['loss']) + 1)
    epochs_cvae = range(1, len(cvae_loss_history['loss']) + 1)
    
    # æ€»æŸå¤±å¯¹æ¯”
    axes[0].plot(epochs_vae, vae_loss_history['loss'], 'b-', linewidth=2, label='VAE_NB')
    axes[0].plot(epochs_cvae, cvae_loss_history['loss'], 'r-', linewidth=2, label='CVAE_NB')
    axes[0].set_title('Total Loss Comparison')
    axes[0].set_xlabel('Epoch')
    axes[0].set_ylabel('Loss')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # é‡æž„æŸå¤±å¯¹æ¯”
    axes[1].plot(epochs_vae, vae_loss_history['recon'], 'b-', linewidth=2, label='VAE_NB')
    axes[1].plot(epochs_cvae, cvae_loss_history['recon'], 'r-', linewidth=2, label='CVAE_NB')
    axes[1].set_title('Reconstruction Loss Comparison')
    axes[1].set_xlabel('Epoch')
    axes[1].set_ylabel('Reconstruction Loss')
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    # KLæ•£åº¦å¯¹æ¯”
    axes[2].plot(epochs_vae, vae_loss_history['kl'], 'b-', linewidth=2, label='VAE_NB')
    axes[2].plot(epochs_cvae, cvae_loss_history['kl'], 'r-', linewidth=2, label='CVAE_NB')
    axes[2].set_title('KL Divergence Comparison')
    axes[2].set_xlabel('Epoch')
    axes[2].set_ylabel('KL Divergence')
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(f'{RESULTS_DIR}/training_loss_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

# ### è½®å»“ç³»æ•°è¯„ä¼°
def calculate_silhouette_scores(adata, representations, labels_key='celltype'):
    """è®¡ç®—ä¸åŒè¡¨ç¤ºæ–¹æ³•çš„è½®å»“ç³»æ•°"""
    scores = {}
    
    if labels_key not in adata.obs.columns:
        print(f"âš ï¸ æ ‡ç­¾ '{labels_key}' ä¸å­˜åœ¨")
        return scores
    
    labels = adata.obs[labels_key].values
    
    for rep_name in representations:
        if rep_name in adata.obsm:
            try:
                rep_data = adata.obsm[rep_name]
                score = silhouette_score(rep_data, labels, metric='euclidean')
                scores[rep_name] = score
                print(f"ðŸ“Š {rep_name}: Silhouette Score = {score:.4f}")
            except Exception as e:
                print(f"âš ï¸ è®¡ç®— {rep_name} çš„è½®å»“ç³»æ•°å¤±è´¥: {e}")
    
    return scores

# è®¡ç®—è½®å»“ç³»æ•°
representation_methods = {
    'X_pca': 'Scanpy PCA',
    'X_vae_nb_std': 'VAE_NB',
    'X_cvae_nb_std': 'CVAE_NB'
}

if 'X_pca_harmony' in adata_scanpy.obsm:
    representation_methods['X_pca_harmony'] = 'Harmony'

print("ðŸ“ˆ è®¡ç®—è½®å»“ç³»æ•°ï¼ˆåŸºäºŽç»†èƒžç±»åž‹ï¼‰:")
silhouette_scores = calculate_silhouette_scores(adata_scanpy, representation_methods.keys(), 'celltype')

# å¯è§†åŒ–è½®å»“ç³»æ•°å¯¹æ¯”
if silhouette_scores:
    methods = [representation_methods[k] for k in silhouette_scores.keys()]
    scores = list(silhouette_scores.values())
    
    plt.figure(figsize=(10, 6))
    bars = plt.bar(methods, scores, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(methods)])
    plt.title('Silhouette Score Comparison (Cell Type Separation)', fontsize=14)
    plt.ylabel('Silhouette Score')
    plt.xticks(rotation=45)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar, score in zip(bars, scores):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                f'{score:.3f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.savefig(f'{RESULTS_DIR}/silhouette_score_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

# ### ä¿å­˜æœ€ç»ˆç»“æžœ
adata_scanpy.write_h5ad(f'{RESULTS_DIR}/integrated_analysis_results.h5ad')
print(f"ðŸ’¾ åˆ†æžç»“æžœå·²ä¿å­˜åˆ°: {RESULTS_DIR}/integrated_analysis_results.h5ad")

# ç”Ÿæˆåˆ†æžæŠ¥å‘Š
print("\nðŸ“‹ åˆ†æžæ€»ç»“:")
print("="*50)
print(f"ðŸ“Š æ•°æ®é›†: {adata_counts.shape[0]} ä¸ªç»†èƒž, {adata_counts.shape[1]} ä¸ªåŸºå› ")
print(f"ðŸ§ª æ ·æœ¬æ•°: {len(adata_counts.obs['sample'].unique())}")
print(f"ðŸ·ï¸ ç»†èƒžç±»åž‹æ•°: {len(adata_counts.obs['celltype'].unique())}")
print("\nðŸ”¬ åˆ†æžæ–¹æ³•:")
for method_key, method_name in representation_methods.items():
    if method_key in adata_scanpy.obsm:
        print(f"  âœ… {method_name}")
    else:
        print(f"  âŒ {method_name} (æœªè¿è¡Œ)")

print(f"\nðŸ“ ç»“æžœæ–‡ä»¶ä¿å­˜åœ¨: {RESULTS_DIR}/")
print("   - UMAPå¯è§†åŒ–å›¾ç‰‡")
print("   - è®­ç»ƒæŸå¤±æ›²çº¿")
print("   - è½®å»“ç³»æ•°å¯¹æ¯”")
print("   - å®Œæ•´åˆ†æžç»“æžœ (.h5ad)")
print("="*50)